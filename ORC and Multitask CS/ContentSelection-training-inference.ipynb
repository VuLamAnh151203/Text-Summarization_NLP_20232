{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff73ceaf3f344ad5846f5314af5334ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c33a6ff0f13a46108ee0adf9cfff26bb",
              "IPY_MODEL_7a0ee5dbbd074eb5a11d0106a1c262b5",
              "IPY_MODEL_ef69c2f1d94742608605a549143af014"
            ],
            "layout": "IPY_MODEL_55bcba950c9b4cff9d356739fe5ac81b"
          }
        },
        "c33a6ff0f13a46108ee0adf9cfff26bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b18ce2f1af84842a089237db8953a0e",
            "placeholder": "​",
            "style": "IPY_MODEL_8431e7a6b92e4982b46f205fa893c159",
            "value": "100%"
          }
        },
        "7a0ee5dbbd074eb5a11d0106a1c262b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72980e29cc3049c287575eaa54571b83",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81a580dcc2de4606af30368ca45ab705",
            "value": 6
          }
        },
        "ef69c2f1d94742608605a549143af014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8112214c35e44a2390a86eb1c27bd755",
            "placeholder": "​",
            "style": "IPY_MODEL_cfddc41d3881416bae3be6be4daf68a4",
            "value": " 6/6 [01:00&lt;00:00, 11.01s/it]"
          }
        },
        "55bcba950c9b4cff9d356739fe5ac81b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b18ce2f1af84842a089237db8953a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8431e7a6b92e4982b46f205fa893c159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72980e29cc3049c287575eaa54571b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a580dcc2de4606af30368ca45ab705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8112214c35e44a2390a86eb1c27bd755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfddc41d3881416bae3be6be4daf68a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "CoOX37rwdXDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "from datasets import load_dataset, Dataset\n",
        "nltk.download('punkt')\n",
        "from transformers import AutoTokenizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtxhNdp3dOey",
        "outputId": "9b2c2e02-ffbf-4e75-f1d2-517f4ba20ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Specify model"
      ],
      "metadata": {
        "id": "mtk6_2SCjikF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_hidden_size, num_layers, dropout, device):\n",
        "        super(HierarchicalGRU, self).__init__()\n",
        "        self.device          = device\n",
        "        self.vocab_size      = vocab_size\n",
        "        self.embedding_dim   = embedding_dim\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "\n",
        "        # word-level GRU layer: word-embeddings -> utterance representation\n",
        "        # divide by 2 becuase bi-directional\n",
        "        self.gru_wlevel = nn.GRU(input_size=self.embedding_dim, hidden_size=int(self.rnn_hidden_size/2), num_layers=num_layers,\n",
        "                                bias=True, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "\n",
        "        # utterance-level GRU layer (with  binary gate)\n",
        "        self.gru_ulevel = nn.GRU(input_size=self.rnn_hidden_size, hidden_size=int(self.rnn_hidden_size/2), num_layers=num_layers,\n",
        "                                bias=True, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "\n",
        "    def forward(self, input, u_len, w_len):\n",
        "        # input => [batch_size, num_utterances, num_words]\n",
        "        # embed => [batch_size, num_utterances, num_words, embedding_dim]\n",
        "        # embed => [batch_size*num_utterances,  num_words, embedding_dim]\n",
        "\n",
        "        batch_size     = input.size(0)\n",
        "        num_utterances = input.size(1)\n",
        "        num_words      = input.size(2)\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        embed = embed.view(batch_size*num_utterances, num_words, self.embedding_dim)\n",
        "\n",
        "        # word-level GRU\n",
        "        self.gru_wlevel.flatten_parameters()\n",
        "        w_output, _ = self.gru_wlevel(embed)\n",
        "        w_len = w_len.reshape(-1)\n",
        "\n",
        "        # utterance-level GRU\n",
        "        utt_input = torch.zeros((w_output.size(0), w_output.size(2)), dtype=torch.float).to(self.device)\n",
        "        for idx, l in enumerate(w_len):\n",
        "            utt_input[idx] = w_output[idx, l-1]\n",
        "        utt_input = utt_input.view(batch_size, num_utterances, self.rnn_hidden_size)\n",
        "        self.gru_ulevel.flatten_parameters()\n",
        "        utt_output, _ = self.gru_ulevel(utt_input)\n",
        "\n",
        "        # reshape the output at different levels\n",
        "        # w_output => [batch_size, num_utt, num_words, 2*hidden]\n",
        "        # u_output => [batch_size, num_utt, hidden]\n",
        "        w_output = w_output.view(batch_size, num_utterances, num_words, -1)\n",
        "        w_len    = w_len.view(batch_size, -1)\n",
        "        w2_len   = [None for _ in range(batch_size)]\n",
        "        for bn, _l in enumerate(u_len):\n",
        "            w2_len[bn] = w_len[bn, :_l].sum().item()\n",
        "\n",
        "        w2_output = torch.zeros((batch_size, max(w2_len), w_output.size(-1))).to(self.device)\n",
        "        utt_indices = [[] for _ in range(batch_size)]\n",
        "        for bn, l1 in enumerate(u_len):\n",
        "            x = 0\n",
        "            for j, l2 in enumerate(w_len[bn, :l1]):\n",
        "                w2_output[bn, x:x+l2, :] = w_output[bn, j, :l2, :]\n",
        "                x += l2.item()\n",
        "                utt_indices[bn].append(x-1) # minus one!!\n",
        "        encoder_output_dict = {\n",
        "            'u_output': utt_output, 'u_len': u_len,\n",
        "            'w_output': w2_output, 'w_len': w2_len, 'utt_indices': utt_indices\n",
        "        }\n",
        "        return encoder_output_dict"
      ],
      "metadata": {
        "id": "ZXrnuXcFdRDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderGRU(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_hidden_size, mem_hidden_size,\n",
        "                num_layers, dropout, device):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        self.device      = device\n",
        "        self.vocab_size  = vocab_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        self.mem_hidden_size = mem_hidden_size\n",
        "        self.num_layers  = num_layers\n",
        "        self.dropout     = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.rnn = nn.GRU(embedding_dim, dec_hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.attention_u = nn.Linear(mem_hidden_size, dec_hidden_size)\n",
        "        self.attention_w = nn.Linear(mem_hidden_size, dec_hidden_size)\n",
        "\n",
        "        self.output_layer = nn.Linear(dec_hidden_size+mem_hidden_size, vocab_size, bias=True)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self, target, encoder_output_dict, logsoftmax=True):\n",
        "        u_output = encoder_output_dict['u_output']\n",
        "        u_len    = encoder_output_dict['u_len']\n",
        "        w_output = encoder_output_dict['w_output']\n",
        "        w_len    = encoder_output_dict['w_len']\n",
        "\n",
        "        utt_indices     = encoder_output_dict['utt_indices']\n",
        "\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        embed = self.embedding(target)\n",
        "        # initial hidden state\n",
        "        initial_h = torch.zeros((self.num_layers, batch_size, self.dec_hidden_size), dtype=torch.float).to(self.device)\n",
        "        for bn, l in enumerate(u_len):\n",
        "            initial_h[:,bn,:] = u_output[bn,l-1,:].unsqueeze(0)\n",
        "\n",
        "        self.rnn.flatten_parameters()\n",
        "        rnn_output, _ = self.rnn(embed, initial_h)\n",
        "\n",
        "        # attention mechanism LEVEL --- Utterance (u)\n",
        "        scores_u = torch.bmm(rnn_output, self.attention_u(u_output).permute(0,2,1))\n",
        "        for bn, l in enumerate(u_len):\n",
        "            scores_u[bn,:,l:].fill_(float('-inf'))\n",
        "        scores_u = F.log_softmax(scores_u, dim=-1)\n",
        "\n",
        "        # attention mechanism LEVEL --- Word (w)\n",
        "        scores_w = torch.bmm(rnn_output, self.attention_w(w_output).permute(0,2,1))\n",
        "        for bn, l in enumerate(w_len):\n",
        "            scores_w[bn,:,l:].fill_(float('-inf'))\n",
        "        # scores_w = F.log_softmax(scores_w, dim=-1)\n",
        "        scores_uw = torch.zeros(scores_w.shape).to(self.device)\n",
        "        scores_uw.fill_(float('-inf')) # when doing log-addition\n",
        "\n",
        "        # Utterance -> Word\n",
        "        for bn in range(batch_size):\n",
        "            idx1 = 0\n",
        "            idx2 = 0\n",
        "            end_indices = utt_indices[bn]\n",
        "            start_indices = [0] + [a+1 for a in end_indices[:-1]]\n",
        "            for i in range(len(utt_indices[bn])):\n",
        "                i1 = start_indices[i]\n",
        "                i2 = end_indices[i]+1 # python\n",
        "                scores_uw[bn, :, i1:i2] = scores_u[bn, :, i].unsqueeze(-1) + F.log_softmax(scores_w[bn, :, i1:i2], dim=-1)\n",
        "\n",
        "        scores_uw = torch.exp(scores_uw)\n",
        "        context_vec = torch.bmm(scores_uw, w_output)\n",
        "\n",
        "        dec_output = self.output_layer(torch.cat((context_vec, rnn_output), dim=-1))\n",
        "\n",
        "        if logsoftmax:\n",
        "            dec_output = self.logsoftmax(dec_output)\n",
        "\n",
        "        return dec_output, scores_uw, torch.exp(scores_u)\n",
        "\n",
        "        # FOR multiple GPU training --- cannot have scores_uw (size error)\n",
        "        # return dec_output\n",
        "\n",
        "    def forward_step(self, xt, ht, encoder_output_dict, d_prev=None, eu_prev=None, logsoftmax=True):\n",
        "        u_output = encoder_output_dict['u_output']\n",
        "        u_len    = encoder_output_dict['u_len']\n",
        "        w_output = encoder_output_dict['w_output']\n",
        "        w_len    = encoder_output_dict['w_len']\n",
        "\n",
        "        utt_indices     = encoder_output_dict['utt_indices']\n",
        "\n",
        "        batch_size = xt.size(0)\n",
        "\n",
        "        xt = self.embedding(xt) # xt => [batch_size, 1, input_size]\n",
        "                                # ht => [batch_size, num_layers, hidden_size]\n",
        "\n",
        "        rnn_output, ht1  = self.rnn(xt, ht)\n",
        "\n",
        "        # attention mechanism LEVEL --- Utterance (u)\n",
        "        scores_u = torch.bmm(rnn_output, self.attention_u(u_output).permute(0,2,1))\n",
        "        for bn, l in enumerate(u_len):\n",
        "            scores_u[bn,:,l:].fill_(float('-inf'))\n",
        "        # scores_u = F.log_softmax(scores_u, dim=-1)\n",
        "        scores_u = F.softmax(scores_u, dim=-1)\n",
        "\n",
        "        # attention mechanism LEVEL --- Word (w)\n",
        "        scores_w = torch.bmm(rnn_output, self.attention_w(w_output).permute(0,2,1))\n",
        "        for bn, l in enumerate(w_len):\n",
        "            scores_w[bn,:,l:].fill_(float('-inf'))\n",
        "        scores_uw = torch.zeros(scores_w.shape).to(self.device)\n",
        "        scores_uw.fill_(float('-inf')) # when doing log-addition\n",
        "\n",
        "        # Utterance -> Word\n",
        "        for bn in range(batch_size):\n",
        "            idx1 = 0\n",
        "            idx2 = 0\n",
        "            end_indices = utt_indices[bn]\n",
        "            start_indices = [0] + [a+1 for a in end_indices[:-1]]\n",
        "            for i in range(len(utt_indices[bn])):\n",
        "                i1 = start_indices[i]\n",
        "                i2 = end_indices[i]+1 # python\n",
        "                scores_uw[bn, :, i1:i2] = scores_u[bn, :, i].unsqueeze(-1) * F.softmax(scores_w[bn, :, i1:i2], dim=-1)\n",
        "\n",
        "\n",
        "        # scores_uw = torch.exp(scores_uw)\n",
        "        context_vec = torch.bmm(scores_uw, w_output)\n",
        "        dec_output = self.output_layer(torch.cat((context_vec, rnn_output), dim=-1))\n",
        "\n",
        "\n",
        "        if logsoftmax:\n",
        "            logsm_dec_output = self.logsoftmax(dec_output)\n",
        "            return logsm_dec_output[:,-1,:], ht1, scores_uw, scores_u, dec_output[:,-1,:]\n",
        "\n",
        "        else:\n",
        "            return dec_output[:,-1,:], ht1, scores_uw, scores_u, dec_output[:,-1,:]"
      ],
      "metadata": {
        "id": "qPB5Da_JdX4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EXTLabeller(nn.Module):\n",
        "    def __init__(self, dropout, rnn_hidden_size, device):\n",
        "        super(EXTLabeller, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = nn.Linear(rnn_hidden_size, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1: nn.init.xavier_normal_(p)\n",
        "            else: nn.init.zeros_(p)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, utt_output):\n",
        "        x = self.dropout(utt_output)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return self.sigmoid(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "HWan7HGhdbY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, args, device):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        # Encoder - Hierarchical GRU\n",
        "        self.encoder = HierarchicalGRU(args['vocab_size'], args['embedding_dim'], args['rnn_hidden_size'],\n",
        "                                       num_layers=args['num_layers_enc'], dropout=args['dropout'], device=device)\n",
        "\n",
        "        # Decoder - GRU with attention mechanism\n",
        "        self.decoder = DecoderGRU(args['vocab_size'], args['embedding_dim'], args['rnn_hidden_size'], args['rnn_hidden_size'],\n",
        "                                       num_layers=args['num_layers_dec'], dropout=args['dropout'], device=device)\n",
        "\n",
        "        self.param_init()\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def param_init(self):\n",
        "        # Initialisation\n",
        "        # zero out the bias term\n",
        "        # don't zero out LayerNorm term e.g. transformer_encoder.layers.0.norm1.weight\n",
        "        for name, p in self.encoder.named_parameters():\n",
        "            if p.dim() > 1: nn.init.xavier_normal_(p)\n",
        "            else:\n",
        "                # if name[-4:] == 'bias': p.data.zero_()\n",
        "                if 'bias' in name: nn.init.zeros_(p)\n",
        "        for name, p in self.decoder.named_parameters():\n",
        "            if p.dim() > 1: nn.init.xavier_normal_(p)\n",
        "            else:\n",
        "                # if name[-4:] == 'bias': p.data.zero_()\n",
        "                if 'bias' in name: nn.init.zeros_(p)\n",
        "\n",
        "    def forward(self, input, u_len, w_len, target):\n",
        "        enc_output_dict = self.encoder(input, u_len, w_len)\n",
        "        dec_output, attn_scores, u_attn_scores = self.decoder(target, enc_output_dict)\n",
        "\n",
        "        # compute coverage\n",
        "        # cov_scores = self.attn2cov(attn_scores)\n",
        "        return dec_output, enc_output_dict['u_output'], attn_scores, u_attn_scores\n",
        "\n",
        "        # FOR multiple GPU training --- cannot have scores_uw (size error)\n",
        "        # dec_output = self.decoder(target, enc_output_dict)\n",
        "        # return dec_output\n",
        "\n",
        "    def decode_beamsearch(self, input, u_len, w_len, decode_dict):\n",
        "        \"\"\"\n",
        "        this method is meant to be used at inference time\n",
        "            input = input to the encoder\n",
        "            u_len = utterance lengths\n",
        "            w_len = word lengths\n",
        "            decode_dict:\n",
        "                - k                = beamwidth for beamsearch\n",
        "                - batch_size       = batch_size\n",
        "                - time_step        = max_summary_length\n",
        "                - vocab_size       = 30522 for BERT\n",
        "                - device           = cpu or cuda\n",
        "                - start_token_id   = ID of the start token\n",
        "                - stop_token_id    = ID of the stop token\n",
        "                - alpha            = length normalisation\n",
        "                - length_offset    = length offset\n",
        "                - keypadmask_dtype = torch.bool\n",
        "        \"\"\"\n",
        "        k                = decode_dict['k']\n",
        "        batch_size       = decode_dict['batch_size']\n",
        "        time_step        = decode_dict['time_step']\n",
        "        vocab_size       = decode_dict['vocab_size']\n",
        "        device           = decode_dict['device']\n",
        "        start_token_id   = decode_dict['start_token_id']\n",
        "        stop_token_id    = decode_dict['stop_token_id']\n",
        "        alpha            = decode_dict['alpha']\n",
        "        penalty_ug       = decode_dict['penalty_ug']\n",
        "        # keypadmask_dtype = decode_dict['keypadmask_dtype'] ---> this is causing on the API that checks for torch1.2 (commented out on 11 Jan 2021)\n",
        "\n",
        "        if batch_size != 1: raise ValueError(\"batch size must be 1\")\n",
        "\n",
        "        # create beam array & scores\n",
        "        beams       = [None for _ in range(k)]\n",
        "        beam_scores = np.zeros((k,))\n",
        "\n",
        "        # we should only feed through the encoder just once!!\n",
        "        enc_output_dict = self.encoder(input, u_len, w_len) # memory\n",
        "        u_output = enc_output_dict['u_output']\n",
        "        w_output = enc_output_dict['w_output']\n",
        "        # w_len    = enc_output_dict['w_len']\n",
        "        enc_time_step   = w_output.size(1)\n",
        "        enc_time_step_u = u_output.size(1)\n",
        "\n",
        "        # we run the decoder time_step times (auto-regressive)\n",
        "        tgt_ids = torch.zeros((time_step,), dtype=torch.int64).to(device)\n",
        "        tgt_ids[0] = start_token_id\n",
        "\n",
        "        for i in range(k): beams[i] = tgt_ids\n",
        "\n",
        "        finished_beams = []\n",
        "        finished_beams_scores = []\n",
        "        finished_attn = []\n",
        "        finished_attn_u = []\n",
        "\n",
        "        # initial hidden state\n",
        "        ht = torch.zeros((self.decoder.num_layers, 1, self.decoder.dec_hidden_size), dtype=torch.float).to(self.device)\n",
        "        l = u_len[0]\n",
        "        ht[:,0,:] = u_output[0,l-1,:].unsqueeze(0)\n",
        "\n",
        "        beam_ht = [None for _ in range(k)]\n",
        "        for _k in range(k): beam_ht[_k] = ht.clone()\n",
        "\n",
        "        finish = False\n",
        "\n",
        "        attn_scores_array = [torch.zeros((time_step, enc_time_step)) for _ in range(k)]\n",
        "        attn_scores_u_array = [torch.zeros((time_step, enc_time_step_u)) for _ in range(k)]\n",
        "\n",
        "        for t in range(time_step-1):\n",
        "            if finish: break\n",
        "\n",
        "            decoder_output_t_array = torch.zeros((k*vocab_size,))\n",
        "\n",
        "            for i, beam in enumerate(beams):\n",
        "\n",
        "                # inference decoding\n",
        "                decoder_output, beam_ht[i], attn_scores, attn_scores_u, _ = self.decoder.forward_step(beam[t:t+1].unsqueeze(0), beam_ht[i], enc_output_dict, logsoftmax=True)\n",
        "\n",
        "                attn_scores_array[i][t, :] = attn_scores[0,0,:]\n",
        "                attn_scores_u_array[i][t, :] = attn_scores_u[0,0,:]\n",
        "                # check if there is STOP_TOKEN emitted in the previous time step already\n",
        "                # i.e. if the input at this time step is STOP_TOKEN\n",
        "                if beam[t] == stop_token_id: # already stop\n",
        "                    decoder_output[0, :] = float('-inf')\n",
        "                    decoder_output[0, stop_token_id] = 0.0 # to ensure STOP_TOKEN will be picked again!\n",
        "\n",
        "                decoder_output_t_array[i*vocab_size:(i+1)*vocab_size] = decoder_output[0]\n",
        "\n",
        "                # add previous beam score bias\n",
        "                decoder_output_t_array[i*vocab_size:(i+1)*vocab_size] += beam_scores[i]\n",
        "\n",
        "                if penalty_ug > 0.0:\n",
        "                    # Penalty term for repeated uni-gram\n",
        "                    unigram_dict = {}\n",
        "                    for tt in range(t+1):\n",
        "                        v = beam[tt].cpu().numpy().item()\n",
        "                        if v not in unigram_dict: unigram_dict[v] = 1\n",
        "                        else: unigram_dict[v] += 1\n",
        "                    for vocab_id, vocab_count in unigram_dict.items():\n",
        "                        decoder_output_t_array[(i*vocab_size)+vocab_id] -= penalty_ug*vocab_count/(t+1)\n",
        "\n",
        "                # only support batch_size = 1!\n",
        "                if t == 0:\n",
        "                    decoder_output_t_array[(i+1)*vocab_size:] = float('-inf')\n",
        "                    break\n",
        "\n",
        "\n",
        "            # Argmax\n",
        "            topk_scores, topk_ids = torch.topk(decoder_output_t_array, k, dim=-1)\n",
        "            scores = topk_scores.double().cpu().numpy()\n",
        "            indices = topk_ids.double().cpu().numpy()\n",
        "\n",
        "            new_beams = [torch.zeros((time_step,), dtype=torch.int64).to(device) for _ in range(k)]\n",
        "            new_attn_scores_array = [torch.zeros((time_step, enc_time_step)) for _ in range(k)]\n",
        "            new_attn_scores_u_array = [torch.zeros((time_step, enc_time_step_u)) for _ in range(k)]\n",
        "            new_beam_ht = [None for _ in range(k)]\n",
        "\n",
        "            for c_idx, node in enumerate(indices):\n",
        "\n",
        "                vocab_idx = node % vocab_size\n",
        "                beam_idx  = int(node / vocab_size)\n",
        "\n",
        "                new_beams[c_idx][:t+1] = beams[beam_idx][:t+1]\n",
        "                new_beams[c_idx][t+1]  = vocab_idx\n",
        "\n",
        "                new_beam_ht[c_idx]     = beam_ht[beam_idx]\n",
        "\n",
        "                new_attn_scores_array[c_idx][:t+1 ,:] = attn_scores_array[beam_idx][:t+1 ,:]\n",
        "                new_attn_scores_u_array[c_idx][:t+1 ,:] = attn_scores_u_array[beam_idx][:t+1 ,:]\n",
        "\n",
        "                # if there is a beam that has [END_TOKEN] --- store it\n",
        "                if vocab_idx == stop_token_id:\n",
        "                    finished_beams.append(new_beams[c_idx][:t+1+1])\n",
        "                    finished_beams_scores.append(scores[c_idx] / t**alpha)\n",
        "                    finished_attn.append(new_attn_scores_array[c_idx][:t+1 ,:])\n",
        "                    finished_attn_u.append(new_attn_scores_u_array[c_idx][:t+1 ,:])\n",
        "                    # print(\"beam{}: [{:.5f}]\".format(c_idx, scores[c_idx] / t**alpha), bert_tokenizer.decode(new_beams[c_idx][:t+1+1].cpu().numpy()))\n",
        "                    scores[c_idx] = float('-inf')\n",
        "\n",
        "            beams = new_beams\n",
        "            beam_ht = new_beam_ht\n",
        "            attn_scores_array = new_attn_scores_array\n",
        "            attn_scores_u_array = new_attn_scores_u_array\n",
        "            beam_scores = scores\n",
        "\n",
        "            # print(\"=========================  t = {} =========================\".format(t))\n",
        "            # for ik in range(k):\n",
        "            #     print(\"beam{}: [{:.5f}]\".format(ik, scores[ik]),bert_tokenizer.decode(beams[ik].cpu().numpy()[:t+2]))\n",
        "            # import pdb; pdb.set_trace()\n",
        "\n",
        "        if len(finished_beams_scores) > 0:\n",
        "            max_id = finished_beams_scores.index(max(finished_beams_scores))\n",
        "            summary_ids = finished_beams[max_id].cpu().numpy()\n",
        "            attn_score  = finished_attn[max_id]\n",
        "            attn_score_u  = finished_attn_u[max_id]\n",
        "        else:\n",
        "            summary_ids = beams[0].cpu().numpy()\n",
        "            attn_score  = attn_scores_array[0]\n",
        "            attn_score_u = attn_scores_u_array[0]\n",
        "\n",
        "        return summary_ids, attn_score, attn_score_u"
      ],
      "metadata": {
        "id": "knUzcO1GddFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Training"
      ],
      "metadata": {
        "id": "Lc6WN3f7fdR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'vocab_size' :50265,\n",
        "    'embedding_dim': 256,\n",
        "    'rnn_hidden_size' : 512,\n",
        "    'num_layers_enc' : 2,\n",
        "    'num_layers_dec' : 1,\n",
        "    'dropout' : 0.1,\n",
        "    \"gamma\" : 0.2,\n",
        "    \"summary_length\" : 512,\n",
        "    \"num_words\" : 50,\n",
        "    \"num_utterances\" : 1000,\n",
        "}\n",
        "num_epochs = 1\n",
        "batch_size = 8\n",
        "display_step = 5"
      ],
      "metadata": {
        "id": "1sWJqNbkftPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dataset"
      ],
      "metadata": {
        "id": "1-n6yGPEk-kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"LA1512/train_pubmed_ORC_4096_20k\")[\"train\"]\n",
        "val_dataset = load_dataset(\"LA1512/val_pubmed_ORC_4096_1592\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbUTswkDdy7M",
        "outputId": "480e15fa-2328-4df7-fc4f-f6636d0fdfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def length2mask(length, batch_size, max_len, torch_device):\n",
        "    mask = torch.zeros((batch_size, max_len), dtype=torch.float, device=torch_device)\n",
        "    for bn in range(batch_size):\n",
        "        l = length[bn].item()\n",
        "        mask[bn,:l].fill_(1.0)\n",
        "    return mask\n",
        "\n",
        "def shift_decoder_target(target, tgt_len, torch_device, mask_offset=False):\n",
        "    batch_size = target.size(0)\n",
        "    max_len = target.size(1)\n",
        "    dtype0  = target.dtype\n",
        "\n",
        "    decoder_target = torch.zeros((batch_size, max_len), dtype=dtype0, device=torch_device)\n",
        "    decoder_target[:,:-1] = target.clone().detach()[:,1:]\n",
        "    # decoder_target[:,-1:] = 103 # MASK_TOKEN_ID = 103\n",
        "    # decoder_target[:,-1:] = 0 # add padding id instead of MASK\n",
        "\n",
        "    # mask for shifted decoder target\n",
        "    decoder_mask = torch.zeros((batch_size, max_len), dtype=torch.float, device=torch_device)\n",
        "    if mask_offset:\n",
        "        offset = 10\n",
        "        for bn, l in enumerate(tgt_len):\n",
        "            # decoder_mask[bn,:l-1].fill_(1.0)\n",
        "            # to accommodate like 10 more [MASK] [MASK] [MASK] [MASK],...\n",
        "            if l-1+offset < max_len: decoder_mask[bn,:l-1+offset].fill_(1.0)\n",
        "            else: decoder_mask[bn,:].fill_(1.0)\n",
        "    else:\n",
        "        for bn, l in enumerate(tgt_len):\n",
        "            decoder_mask[bn,:l-1].fill_(1.0)\n",
        "\n",
        "    return decoder_target, decoder_mask"
      ],
      "metadata": {
        "id": "PtZlH2-Pkyk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HierArticleBatcher():\n",
        "    def __init__(self, tokenizer, config,document, torch_device):\n",
        "\n",
        "        self.num_utterances = config['num_utterances']\n",
        "        self.num_words      = config['num_words']\n",
        "        self.summary_length = config['summary_length']\n",
        "        self.max_sum_len = config['summary_length']\n",
        "        self.tokenizer = tokenizer\n",
        "        self.document = document\n",
        "        self.torch_device = torch_device\n",
        "        self.device = torch_device\n",
        "\n",
        "\n",
        "    # Override\n",
        "    def get_a_batch(self, batch_size, batch_index):\n",
        "        \"\"\"\n",
        "        return input, u_len, w_len, target, tgt_len, ext_label\n",
        "        \"\"\"\n",
        "        input = np.zeros((batch_size, self.num_utterances, self.num_words), dtype=np.int64)\n",
        "        u_len = np.zeros((batch_size), dtype=np.int64)\n",
        "        w_len = np.zeros((batch_size, self.num_utterances), dtype=np.int64)\n",
        "        ext_target = np.zeros((batch_size, self.num_utterances), dtype=np.float32)\n",
        "\n",
        "        target  = np.zeros((batch_size, self.summary_length), dtype=np.int64)\n",
        "        target.fill(50264)\n",
        "        tgt_len = np.zeros((batch_size), dtype=np.int64)\n",
        "\n",
        "        batch_count = 0\n",
        "\n",
        "        while batch_count < batch_size:\n",
        "            # ENCODER\n",
        "            sentences = tokenize.sent_tokenize(self.document[batch_count + batch_index*batch_size][\"article\"])\n",
        "            num_sentences = len(sentences)\n",
        "            if num_sentences > self.num_utterances:\n",
        "                num_sentences = self.num_utterances\n",
        "                sentences = sentences[:self.num_utterances]\n",
        "            u_len[batch_count] = num_sentences\n",
        "\n",
        "            for j, sent in enumerate(sentences):\n",
        "                token_ids = self.tokenizer.encode(sent.lower(), add_special_tokens=False, max_length=50000)\n",
        "                utt_len = len(token_ids)\n",
        "                if utt_len > self.num_words:\n",
        "                    utt_len = self.num_words\n",
        "                    token_ids = token_ids[:self.num_words]\n",
        "                input[batch_count,j,:utt_len] = token_ids\n",
        "                w_len[batch_count,j] = utt_len\n",
        "\n",
        "            # Extractive Sum Label\n",
        "            ext_target_example = self.document[batch_count + batch_index*batch_size][\"ext_target\"]\n",
        "            positive_postions = [i for i in range(num_sentences) if ext_target_example[i] == 1]\n",
        "            ext_target[batch_count][positive_postions] = 1.0\n",
        "\n",
        "            # DECODER\n",
        "            description   =  \" \".join(self.document[batch_count + batch_index*batch_size][\"abstract\"]).lower()\n",
        "            concat_tokens = [0]\n",
        "            sentences = tokenize.sent_tokenize(description)\n",
        "            for j, sent in enumerate(sentences):\n",
        "                token_ids = self.tokenizer.encode(sent, add_special_tokens=False, max_length=50000)\n",
        "                concat_tokens.extend(token_ids)\n",
        "                concat_tokens.extend([1]) # [SEP]\n",
        "            tl = len(concat_tokens)\n",
        "            if tl > self.summary_length:\n",
        "                concat_tokens = concat_tokens[:self.summary_length]\n",
        "                tl = self.summary_length\n",
        "            target[batch_count, :tl] = concat_tokens\n",
        "            tgt_len[batch_count] = tl\n",
        "\n",
        "            # increment\n",
        "            batch_count += 1\n",
        "\n",
        "        u_len_max = u_len.max()\n",
        "        w_len_max = w_len.max()\n",
        "\n",
        "        input = torch.from_numpy(input[:, :u_len_max, :w_len_max]).to(self.device)\n",
        "        u_len = torch.from_numpy(u_len).to(self.device)\n",
        "        w_len = torch.from_numpy(w_len[:, :u_len_max]).to(self.device)\n",
        "        target = torch.from_numpy(target).to(self.device)\n",
        "        ext_target = torch.from_numpy(ext_target[:, :u_len_max]).to(self.device)\n",
        "\n",
        "        return input, u_len, w_len, target, tgt_len, ext_target"
      ],
      "metadata": {
        "id": "aR4VlOqNkzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, ext_labeller, gamma, val_batcher, batch_size, config, torch_device):\n",
        "    print(\"start validating\")\n",
        "    criterion = nn.NLLLoss(reduction='none')\n",
        "    ext_criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "    eval_total_loss1 = 0.0\n",
        "    eval_total_loss2 = 0.0\n",
        "    eval_total_tokens1 = 0\n",
        "    eval_total_tokens2 = 0\n",
        "\n",
        "    num_batch = int(len(data_val_small) / batch_size)\n",
        "    for batch_index in range(num_batch):\n",
        "    # for i in range(5):\n",
        "        input, u_len, w_len, target, tgt_len, ext_target = val_batcher.get_a_batch(batch_size, batch_index)\n",
        "\n",
        "        # decoder target\n",
        "        decoder_target, decoder_mask = shift_decoder_target(target, tgt_len, torch_device)\n",
        "        decoder_target = decoder_target.view(-1)\n",
        "        decoder_mask = decoder_mask.view(-1)\n",
        "        decoder_output, enc_u_output, _, _ = model(input, u_len, w_len, target)\n",
        "\n",
        "        loss1 = criterion(decoder_output.view(-1, config['vocab_size']), decoder_target)\n",
        "\n",
        "        loss_utt_mask = length2mask(u_len, batch_size, u_len.max().item(), torch_device)\n",
        "        ext_output = ext_labeller(enc_u_output).squeeze(-1)\n",
        "        loss2 = ext_criterion(ext_output, ext_target)\n",
        "\n",
        "        eval_total_loss1   += (loss1 * decoder_mask).sum().item()\n",
        "        eval_total_loss2  += (loss2 * loss_utt_mask).sum().item()\n",
        "\n",
        "        eval_total_tokens1 += decoder_mask.sum().item()\n",
        "        eval_total_tokens2 += loss_utt_mask.sum().item()\n",
        "\n",
        "        print(\"#\", end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "    avg_eval_loss1 = eval_total_loss1 / eval_total_tokens1\n",
        "    avg_eval_loss2 = eval_total_loss2 / eval_total_tokens2\n",
        "    val_batcher.epoch_counter = 0\n",
        "    val_batcher.cur_id = 0\n",
        "    print(\"finish validating\")\n",
        "    avg_eval_loss = (1-gamma)*avg_eval_loss1 + gamma*avg_eval_loss2\n",
        "    print(avg_eval_loss)\n",
        "    return avg_eval_loss"
      ],
      "metadata": {
        "id": "8HKR5iqsvgyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the loss function\n",
        "criterion = nn.NLLLoss(reduction='none')\n",
        "ext_criterion = nn.BCELoss(reduction='none')"
      ],
      "metadata": {
        "id": "5XjlCD0yeggB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "rn7RaNPvoL-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the model\n",
        "\n",
        "model = EncoderDecoder(config, device= torch_device)\n",
        "ext_labeller = EXTLabeller(rnn_hidden_size=config['rnn_hidden_size'], dropout=config['dropout'], device=torch_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI9g5vYAxU2O",
        "outputId": "404aad12-31fa-483d-851a-d03f84219ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batch = int(len(data_small) / batch_size)"
      ],
      "metadata": {
        "id": "M1GZRQcExro-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the optimizer\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)\n",
        "optimizer.zero_grad()\n",
        "ext_optimizer = optim.Adam(ext_labeller.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)\n",
        "ext_optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "eoqvEv-dxYkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YMie15fSWSK",
        "outputId": "068b679e-7fdd-42b9-ccec-5cc3c0a56fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"pszemraj/led-base-book-summary\")\n",
        "\n",
        "\n",
        "batcher = HierArticleBatcher(tokenizer, config,data_small ,torch_device)\n",
        "val_batcher = HierArticleBatcher(tokenizer, config,data_val_small ,torch_device)\n",
        "\n",
        "\n",
        "if torch_device == 'cuda':\n",
        "        model.cuda()\n",
        "        ext_labeller.cuda()\n",
        "\n",
        "\n",
        "model.train()\n",
        "step = 1\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_index in range(num_batch):\n",
        "        input, u_len, w_len, target, tgt_len, ext_target = batcher.get_a_batch(batch_size,batch_index)\n",
        "\n",
        "        # decoder target\n",
        "        decoder_target, decoder_mask = shift_decoder_target(target, tgt_len, torch_device, mask_offset=False)\n",
        "        decoder_target = decoder_target.view(-1)\n",
        "        decoder_mask = decoder_mask.view(-1)\n",
        "\n",
        "        # Forward pass\n",
        "        decoder_output, enc_u_output, attn_scores, u_attn_scores = model(input, u_len, w_len, target)\n",
        "\n",
        "        # Multitask Learning: Task 1 - Predicting targets\n",
        "        loss1 = criterion(decoder_output.view(-1, config['vocab_size']), decoder_target)\n",
        "        loss1 = (loss1 * decoder_mask).sum() / decoder_mask.sum()\n",
        "\n",
        "        # Multitask Learning: Task 2 - Extractive Summarisation\n",
        "        loss_utt_mask = length2mask(u_len, batch_size, u_len.max().item(), torch_device)\n",
        "        ext_output = ext_labeller(enc_u_output).squeeze(-1)\n",
        "        loss2 = ext_criterion(ext_output, ext_target)\n",
        "        loss2 = (loss2 * loss_utt_mask).sum() / loss_utt_mask.sum()\n",
        "\n",
        "        loss = (1-config[\"gamma\"])*loss1 + config[\"gamma\"]*loss2\n",
        "\n",
        "\n",
        "        if step % display_step == 0:\n",
        "          print(f\"Step: {step} The loss: {loss / step}\")\n",
        "\n",
        "        step += 1\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    evaluate(model, ext_labeller, config[\"gamma\"],val_batcher, batch_size, config, torch_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ff73ceaf3f344ad5846f5314af5334ce",
            "c33a6ff0f13a46108ee0adf9cfff26bb",
            "7a0ee5dbbd074eb5a11d0106a1c262b5",
            "ef69c2f1d94742608605a549143af014",
            "55bcba950c9b4cff9d356739fe5ac81b",
            "3b18ce2f1af84842a089237db8953a0e",
            "8431e7a6b92e4982b46f205fa893c159",
            "72980e29cc3049c287575eaa54571b83",
            "81a580dcc2de4606af30368ca45ab705",
            "8112214c35e44a2390a86eb1c27bd755",
            "cfddc41d3881416bae3be6be4daf68a4"
          ]
        },
        "id": "T_OUPuzTxFtv",
        "outputId": "4e9f1675-6e72-4ca5-bc1b-f91f3c388a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff73ceaf3f344ad5846f5314af5334ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 5 The loss: 1.7597471475601196\n",
            "start validating\n",
            "######\n",
            "finish validating\n",
            "8.798362437948713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdFhZ5vsbqXa",
        "outputId": "7c1a6092-f1e7-4274-b493-3b2b1432fc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Inference"
      ],
      "metadata": {
        "id": "f1LRWXJORABT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"inference_mode\" : \"mcs\",\n",
        "    \"max_abssum_len\" : 4096,\n",
        "    \"max_num_sent\"   : 1000,\n",
        "    \"max_word_in_sent\" : 120,\n",
        "    \"beam_width\" : 4,\n",
        "    \"time_step\" : 512,\n",
        "    \"penalty_ug\" : 0,\n",
        "    \"alpha\" : 1.25,\n",
        "    \"length_offset\" : 5,\n",
        "    \"use_gpu\" : True\n",
        "}"
      ],
      "metadata": {
        "id": "2ZGXm8WqYqs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_enc_input(tokenizer, list_sentences,\n",
        "        max_num_sent, max_word_in_sent, use_gpu=True):\n",
        "\n",
        "    batch_size = len(list_sentences)\n",
        "    input = np.zeros((batch_size, max_num_sent, max_word_in_sent), dtype=np.int64)\n",
        "    u_len = np.zeros((batch_size), dtype=np.int64)\n",
        "    w_len = np.zeros((batch_size, max_num_sent), dtype=np.int64)\n",
        "\n",
        "    for i, sentences in enumerate(list_sentences):\n",
        "        num_sentences = len(sentences)\n",
        "        if num_sentences > max_num_sent:\n",
        "            num_sentences = max_num_sent\n",
        "            sentences = sentences[:max_num_sent]\n",
        "        u_len[i] = num_sentences\n",
        "\n",
        "        for j, sent in enumerate(sentences):\n",
        "            token_ids = tokenizer.encode(sent, max_length=500000)[1:-1] # remove [CLS], [SEP]\n",
        "            utt_len = len(token_ids)\n",
        "            if utt_len > max_word_in_sent:\n",
        "                utt_len = max_word_in_sent\n",
        "                token_ids = token_ids[:max_word_in_sent]\n",
        "            input[i,j,:utt_len] = token_ids\n",
        "            w_len[i,j] = utt_len\n",
        "    input = torch.from_numpy(input)\n",
        "    u_len = torch.from_numpy(u_len)\n",
        "    w_len = torch.from_numpy(w_len)\n",
        "\n",
        "    if use_gpu:\n",
        "        input = input.cuda()\n",
        "        u_len = u_len.cuda()\n",
        "        w_len = w_len.cuda()\n",
        "\n",
        "    return input, u_len, w_len\n"
      ],
      "metadata": {
        "id": "bnLEqAQ1UF1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_utt_attn_without_ref(model, enc_batch, beam_width=4, time_step=240,\n",
        "                            penalty_ug=0.0, alpha=1.25, length_offset=5, torch_device='cpu'):\n",
        "    decode_dict = {\n",
        "        'k': beam_width,\n",
        "        'time_step': time_step,\n",
        "        'vocab_size': 50265,\n",
        "        'device': torch_device,\n",
        "        'start_token_id': 0, 'stop_token_id': 2,\n",
        "        'alpha': alpha,\n",
        "        'length_offset': length_offset,\n",
        "        'penalty_ug': penalty_ug,\n",
        "        'keypadmask_dtype': torch.bool,\n",
        "        'memory_utt': False,\n",
        "        'batch_size': 1\n",
        "    }\n",
        "    # batch_size should be 1\n",
        "    with torch.no_grad():\n",
        "        summary_ids, attn_scores, u_attn_scores = model.decode_beamsearch(\n",
        "                enc_batch[\"input\"], enc_batch[\"u_len\"], enc_batch[\"w_len\"], decode_dict)\n",
        "\n",
        "    N = enc_batch[\"u_len\"][0].item()\n",
        "    attention = u_attn_scores[:,:N].sum(dim=0) / u_attn_scores[:,:N].sum()\n",
        "    attention = attention.cpu().numpy()\n",
        "    return attention"
      ],
      "metadata": {
        "id": "AyPKYPZkUxlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ranking_score(score):\n",
        "    \"\"\"\n",
        "        the item with lowest rank gets 0.0, the item with highest rank gets 1.0,\n",
        "        and everthing else gets the value in between 0.0 and 1.0\n",
        "    \"\"\"\n",
        "    rank_ascending = np.argsort(score)\n",
        "    N = len(score)\n",
        "    if N == 1: return np.array([1.0])\n",
        "    ranking_score = [None for _ in range(N)]\n",
        "    for i, idx in enumerate(rank_ascending):\n",
        "        ranking_score[idx] = i/(N-1)\n",
        "    return np.array(ranking_score)"
      ],
      "metadata": {
        "id": "l0_4jwqiVN1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def inference_hiermodel(args, data):\n",
        "    inference_mode = args['inference_mode']\n",
        "\n",
        "    # uses GPU in training or not\n",
        "    if torch.cuda.is_available() and args['use_gpu']: torch_device = 'cuda'\n",
        "    else: torch_device = 'cpu'\n",
        "    use_gpu = args['use_gpu']\n",
        "\n",
        "    # ----- Hierarchical Model Configurations ----- #\n",
        "    # TODO: replace this part to be not hard coded\n",
        "    args['num_utterances'] = args['max_num_sent']\n",
        "    args['num_words']      = args['max_word_in_sent']\n",
        "\n",
        "    args['vocab_size']     = 50265 # BERT tokenizer\n",
        "    args['embedding_dim']   = 256   # word embeeding dimension\n",
        "    args['rnn_hidden_size'] = 512 # RNN hidden size\n",
        "    args['dropout']        = 0.0\n",
        "    args['num_layers_enc'] = 2    # in total it's num_layers_enc*2 (word/utt)\n",
        "    args['num_layers_dec'] = 1\n",
        "    # --------------------------------------------- #\n",
        "\n",
        "    # Load the model\n",
        "    trained_model_path = args['load']\n",
        "    if use_gpu:\n",
        "        state = torch.load(trained_model_path)\n",
        "    else:\n",
        "        state = torch.load(trained_model_path, map_location=torch.device('cpu'))\n",
        "    model = EncoderDecoder(args, device=torch_device)\n",
        "    model_state_dict = state['model']\n",
        "    model.load_state_dict(model_state_dict)\n",
        "\n",
        "    ext_labeller = EXTLabeller(rnn_hidden_size=args['rnn_hidden_size'], dropout=args['dropout'], device=torch_device)\n",
        "    if inference_mode in ['ext', 'mcs']:\n",
        "        ext_labeller_state_dict = state['ext_labeller']\n",
        "        ext_labeller.load_state_dict(ext_labeller_state_dict)\n",
        "    model.eval()\n",
        "    ext_labeller.eval()\n",
        "    print('model loaded!')\n",
        "\n",
        "\n",
        "    tokenizer  = AutoTokenizer.from_pretrained(\"pszemraj/led-base-book-summary\")\n",
        "\n",
        "    # # data\n",
        "    # # datapath = \"podcast_sum0/lib/test_data/podcast_testset.bin\"\n",
        "    # # datapath = \"arxiv_sum0/lib/data/arxiv_test.pk.bin\"\n",
        "    # # datapath = \"arxiv_sum0/lib/pubmed_data/pubmed_test.pk.bin\"\n",
        "    # datapath = args['datapath']\n",
        "    # print(\"datapath =\", datapath)\n",
        "    # with open(datapath, 'rb') as f:\n",
        "    #     data = pickle.load(f, encoding=\"bytes\")\n",
        "    # print(\"len(data) = {}\".format(len(data)))\n",
        "\n",
        "    # ids = [x for x in range(start_id, end_id)]\n",
        "    # if args['random_order']: random.shuffle(ids)\n",
        "\n",
        "    # inference parameters\n",
        "    beam_width    = args['beam_width']\n",
        "    time_step     = args['time_step']\n",
        "    penalty_ug    = args['penalty_ug']\n",
        "    alpha         = args['alpha']\n",
        "    length_offset = args['length_offset']\n",
        "\n",
        "\n",
        "    data_list = data.to_list()\n",
        "\n",
        "    for doc in tqdm(data_list):\n",
        "\n",
        "        sentences = tokenize.sent_tokenize(doc[\"article\"])\n",
        "\n",
        "        num_sent = len(sentences)\n",
        "\n",
        "        try: l1 = len(tokenizer.encode(doc[\"article\"], max_length=500000))\n",
        "        except IndexError: l1 = 0\n",
        "\n",
        "        # the length is within the limit --> no selection needed\n",
        "        if l1 < args['max_abssum_len']:\n",
        "            filtered_sentences = sentences\n",
        "        # perform MCS\n",
        "        else:\n",
        "            keep_idx = []\n",
        "            input, u_len, w_len = get_enc_input(tokenizer, [sentences], args['max_num_sent'],\n",
        "                                                args['max_word_in_sent'], use_gpu=use_gpu)\n",
        "            # ------ MODULE1: Extractive Sum ------ #\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                encoder_output_dict = model.encoder(input, u_len, w_len)\n",
        "                enc_u_output = encoder_output_dict['u_output']\n",
        "                ext_output = ext_labeller(enc_u_output).squeeze(-1)\n",
        "            ext_output = ext_output[0].cpu().numpy()\n",
        "            # -------------- END MODULE1 --------------- #\n",
        "            # ------ MODULE2: Sentence-Level Attn ------ #\n",
        "            batch = {\"input\": input, \"u_len\": u_len, \"w_len\": w_len}\n",
        "            attention = get_utt_attn_without_ref(model, batch, beam_width=beam_width, time_step=time_step,\n",
        "                        penalty_ug=penalty_ug, alpha=alpha, length_offset=length_offset, torch_device=torch_device)\n",
        "            if len(sentences) != attention.shape[0]:\n",
        "                if len(sentences) > args['max_num_sent']:\n",
        "                    sentences = sentences[:args['max_num_sent']]\n",
        "                else:\n",
        "                    raise ValueError(\"shape error #1\")\n",
        "            # -------------- END MODULE2 --------------- #\n",
        "            N1 = len(attention)\n",
        "            N2 = len(ext_output)\n",
        "            if N2 > N1: ext_output = ext_output[:N1]\n",
        "            ext_score = compute_ranking_score(ext_output)\n",
        "            attn_score = compute_ranking_score(attention)\n",
        "\n",
        "            if inference_mode == 'mcs':\n",
        "                # taking geometric mean --- (simple mean works too!)\n",
        "                total_score = np.sqrt(attn_score * ext_score)\n",
        "            elif inference_mode == 'ext':  total_score = ext_score\n",
        "            elif inference_mode == 'attn': total_score = attn_score\n",
        "            else: raise Exception(\"inference mode error!\")\n",
        "\n",
        "            rank = np.argsort(total_score)[::-1]\n",
        "            keep_idx = []\n",
        "            total_length = 0\n",
        "            for sent_i in rank:\n",
        "                if total_length < args['max_abssum_len']:\n",
        "                    sent = sentences[sent_i]\n",
        "                    length = len(tokenizer.encode(sent, max_length=50000)[1:-1]) # ignore <s> and </s>\n",
        "                    total_length += length\n",
        "                    keep_idx.append(sent_i)\n",
        "                else:\n",
        "                    break\n",
        "            keep_idx = sorted(keep_idx)\n",
        "            filtered_sentences = [sentences[j] for j in keep_idx]\n",
        "        doc[\"article_MCS\"] = \" \".join(filtered_sentences)\n",
        "\n",
        "    return Dataset.from_list(data_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "iArIqjVdRj4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_hiermodel(args, data_val_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvw4I7JTZyU7",
        "outputId": "9eb0eaa5-865c-4944-ca0a-389a57df482e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 50/50 [12:35<00:00, 15.11s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['article', 'abstract', 'section_names', 'article_CS', 'ext_target', 'article_MCS'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}